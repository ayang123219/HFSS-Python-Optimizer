"""HFSS AIä»£ç†ä¼˜åŒ–å™¨ - ä¸»åŠ¨å­¦ä¹ æ¡†æ¶
åŸºäºä¸»åŠ¨å­¦ä¹ å’Œæ··åˆä»£ç†æ¨¡å‹ï¼ˆGPR+DNNï¼‰çš„HFSSå¤©çº¿å‚æ•°ä¼˜åŒ–ç³»ç»Ÿ
ç›®æ ‡ï¼šå®ç°å¯¹å¤©çº¿å‚æ•°çš„ä¼˜åŒ–ï¼Œç»™å‡ºèƒ½å¤Ÿæ»¡è¶³çº¦æŸæ¡ä»¶çš„æœ€ä¼˜å‚æ•°ç»„åˆ
_æ ¸å¿ƒåŠŸèƒ½ä¸æµç¨‹_ï¼š
ä¸€. åˆå§‹æ•°æ®é›†æ„å»ºâ€‹â€‹
â€‹    1.â€‹æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·â€‹â€‹ï¼šç”Ÿæˆåˆå§‹å‚æ•°ç»„åˆï¼ˆå¦‚å¤©çº¿å°ºå¯¸ã€ææ–™å±æ€§ç­‰è®¾å®šå¥½çš„å‚æ•°å˜é‡ï¼‰ï¼Œç¡®ä¿è®¾è®¡ç©ºé—´å‡åŒ€è¦†ç›–ã€‚
â€‹    2.â€‹HFSSä»¿çœŸä¸ç‰¹å¾æå–â€‹â€‹ï¼š
        æ¯ä¸ªå‚æ•°ç»„åˆé€šè¿‡HFSSä»¿çœŸè·å¾—ä»¿çœŸç»“æœï¼ˆå¦‚å¤æ•°Så‚æ•°ï¼‰
        é™é‡‡æ ·æå–ç‰¹å¾å‘é‡ã€‚
        å­˜å‚¨ä¸ºæ•°æ®é›†ï¼š(è¾“å…¥å‚æ•°, ç‰¹å¾å‘é‡)ã€‚
â€‹    3.â€‹å…³é”®æ”¹è¿›â€‹â€‹ï¼š
        ç‰¹å¾å‘é‡èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šä»£è¡¨å¤©çº¿çš„ç”µç£ç‰¹æ€§ï¼ˆå¦‚å¤æ•°çš„Så‚æ•°ï¼‰ï¼Œç¡®ä¿æ¨¡å‹æ³›åŒ–æ€§ã€‚
        æ¨¡å‹è®­ç»ƒçš„æ•°æ®é›†ä¸çº¦æŸå‡½æ•°ç›¸äº’ç‹¬ç«‹ä¸å…³è”
â€‹äºŒ. GPRä»£ç†æ¨¡å‹è®­ç»ƒâ€‹â€‹
â€‹    1.â€‹è¾“å…¥â€‹â€‹ï¼šè®¾è®¡å‚æ•°ï¼ˆå¦‚è´´ç‰‡é•¿åº¦ã€é¦ˆç”µä½ç½®ï¼‰ã€‚
    2.â€‹è¾“å‡ºâ€‹â€‹ï¼šå¤šç»´ç‰¹å¾å‘é‡ï¼ˆå¦‚S11éšé¢‘ç‡å˜åŒ–çš„å¤æ•°åºåˆ—ï¼‰ã€‚
    3.â€‹è®­ç»ƒç›®æ ‡â€‹â€‹ï¼šæœ€å°åŒ–é¢„æµ‹ç‰¹å¾å‘é‡ä¸çœŸå®ä»¿çœŸçš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼Œä¸æ¶‰åŠçº¦æŸæ¡ä»¶ã€‚
ä¸‰. ä¸»åŠ¨å­¦ä¹ å¾ªç¯â€‹â€‹
â€‹â€‹æ¯ä¸€è½®è¿­ä»£æµç¨‹å¦‚ä¸‹ï¼šâ€‹â€‹
    1.â€‹ç”Ÿæˆå€™é€‰æ± â€‹â€‹ï¼šéšæœºç”Ÿæˆ1000ç»„å‚æ•°ç»„åˆã€‚
    2.â€‹GPRé¢„æµ‹ä¸ä¸ç¡®å®šæ€§è¯„ä¼°â€‹â€‹ï¼š
        é¢„æµ‹å€™é€‰ç‚¹çš„ç‰¹å¾å‘é‡yå’Œé¢„æµ‹ä¸ç¡®å®šæ€§ï¼ˆæ ‡å‡†å·®ï¼‰ã€‚
    3.â€‹ä¸»åŠ¨å­¦ä¹ æ ·æœ¬ç­›é€‰â€‹â€‹ï¼š
        -â€‹ç­–ç•¥â€‹â€‹ï¼šé€‰æ‹©â€‹â€‹é«˜ä¸ç¡®å®šæ€§â€‹â€‹æˆ–â€‹â€‹é«˜æ€§èƒ½æ½œåŠ›â€‹â€‹æ ·æœ¬ï¼š
â€‹            â€‹é«˜ä¸ç¡®å®šæ€§åŒºåŸŸâ€‹â€‹ï¼šmax(Ïƒ)>Ïƒï¼ˆthresholdï¼‰ï¼ˆæ¢ç´¢æœªå……åˆ†åŒºåŸŸï¼‰ã€‚
â€‹            â€‹é«˜æ€§èƒ½æ½œåŠ›åŒºåŸŸâ€‹â€‹ï¼šP=-Loss æœ€å¤§ï¼ˆæŸå¤±è¶Šå°ï¼Œ-Lossè¶Šå¤§ï¼Œè¶Šæ¥è¿‘çº¦æŸæ¡ä»¶ï¼Œå¦‚S11æœ€æ¥è¿‘ç›®æ ‡é¢‘æ®µï¼‰ã€‚
            è®¡ç®—æŸå¤±çš„æ—¶å€™å®Œå…¨å¯ä»¥å…ˆä»ç‰¹å¾å‘é‡é‡Œè¿”è¿˜ä¸ºå¤æ•°så‚æ•°ï¼Œç„¶åè®¡ç®—å¼å¾—å‡ºdBï¼Œè€Œéä»ç‰¹å¾å‘é‡é‡Œæå–dBçš„ç‰¹å¾å†è®¡ç®—æŸå¤±ï¼Œ
                    ä¿è¯æ¨¡å‹æœ¬èº«å­¦ä¹ çš„æ˜¯æœ¬è´¨çš„Så‚æ•°ï¼Œè€ŒédBçš„ç‰¹å¾ã€‚
        -â€‹â€‹ç­›é€‰å…¬å¼(å½’ä¸€åŒ–)â€‹â€‹ï¼š
            Score=aâ‹…[Ïƒ/Ïƒ(max)] + (1âˆ’a)â‹…[(P-Pmin)/(Pmax-Pmin)], 
                å…¶ä¸­ a å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨(é€šå¸¸ a=0.6), Ïƒ ä¸ºé¢„æµ‹ä¸ç¡®å®šæ€§, 
                Pæ€§èƒ½æ½œåŠ›, Pmax/Pminä¸ºå½“å‰å€™é€‰æ± ä¸­æ€§èƒ½æ½œåŠ›çš„æœ€å¤§å€¼å’Œæœ€å°å€¼ã€‚
â€‹    4.â€‹HFSSéªŒè¯ä¸æ•°æ®é›†æ›´æ–°â€‹â€‹ï¼š
        å¯¹ç­›é€‰å‡ºçš„æ ·æœ¬è¿›è¡ŒHFSSä»¿çœŸï¼Œæå–çœŸå®ç‰¹å¾å‘é‡ï¼ŒåŠ å…¥æ•°æ®é›†ã€‚
â€‹    5.â€‹æ¨¡å‹é‡è®­ç»ƒâ€‹â€‹ï¼šç”¨æ‰©å……åçš„æ•°æ®é›†æ›´æ–°GPRæ¨¡å‹ã€‚
â€‹        â€‹ç»ˆæ­¢æ¡ä»¶â€‹â€‹ï¼š
            è¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼ˆå¦‚20è½®ï¼‰ï¼Œæˆ–GPRçš„MSEæ”¶æ•›ï¼ˆå¦‚å˜åŒ–ç‡<5%ï¼‰ã€‚
â€‹â€‹å››. æœ€ä¼˜è§£æå–ï¼ˆç‹¬ç«‹äºä¸»åŠ¨å­¦ä¹ ï¼‰â€‹â€‹
â€‹    1.â€‹é¢„ç­›é€‰â€‹â€‹ï¼š
        -ç”Ÿæˆ10,000ç»„éšæœºå‚æ•°ï¼Œç”¨GPRé¢„æµ‹ç‰¹å¾å‘é‡ã€‚
        -åŸºäºçº¦æŸæ¡ä»¶ï¼ˆå¯»æ‰¾æŸå¤±å€¼æœ€ä½ï¼‰è¿‡æ»¤å€™é€‰è§£ï¼Œä¿ç•™Top-Kï¼ˆå¦‚50ç»„ï¼‰ã€‚
    2.â€‹HFSSéªŒè¯â€‹â€‹ï¼š
        -å°†å€™é€‰è§£é€å…¥HFSSä»¿çœŸï¼Œå‰”é™¤é¢„æµ‹è¯¯å·®å¤§çš„æ ·æœ¬ã€‚
        -è¾“å‡ºæ‰€æœ‰æ»¡è¶³çº¦æŸçš„è§£ã€‚
    3.â€‹å¤±è´¥å¤„ç†â€‹â€‹ï¼š
        -è‹¥éªŒè¯åæ— è§£ï¼Œå°†å¤±è´¥æ ·æœ¬åŠ å…¥æ•°æ®é›†ï¼Œé‡æ–°è®­ç»ƒGPRå¹¶é‡å¤é¢„ç­›é€‰ã€‚
"""

"""HFSS AIä»£ç†ä¼˜åŒ–å™¨ - ä¸»åŠ¨å­¦ä¹ æ¡†æ¶"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern
from sklearn.preprocessing import StandardScaler
from pyDOE import lhs
import os
import time
import json
import re
import traceback
from typing import List, Dict, Callable, Optional, Tuple
from api import HFSSController
import warnings
from DataSet import HfssDataset
from GPRtrainer import GPRTrainer

# è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei']
plt.rcParams['axes.unicode_minus'] = False

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

class HfssAIAgentOptimizer:
    """HFSS AIä»£ç†ä¼˜åŒ–å™¨ï¼ˆé‡æ„ç‰ˆï¼‰"""
    
    def __init__(self, 
                 project_path: str,
                 design_name: str = "HFSSDesign1",
                 setup_name: str = "Setup1",
                 sweep_name: str = "Sweep",
                 variables: List[dict] = None,
                 freq_range: Tuple[float, float] = (5.5e9, 7e9),
                 constraints: List[dict] = None,
                 global_port_map: Dict[str, Tuple[str, str]] = None,
                 output_dir: str = "optim_results",
                 max_active_cycles: int = 30,   # æœ€å¤§ä¸»åŠ¨å­¦ä¹ è½®æ¬¡
                 init_sample_multiplier: int = 10, # åˆå§‹æ ·æœ¬æ•°é‡çš„å€æ•°
                 feature_freq_points: int = 20, # ç‰¹å¾å‘é‡çš„é¢‘ç‡ç‚¹æ•°é‡
                 n_select: int = 3, # æ¯è½®ä¸»åŠ¨å­¦ä¹ é€‰æ‹©çš„æ ·æœ¬æ•°
                 ei_balance: float = 0.6, # æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡å› å­
                 initial_dataset_path: Optional[str] = None,  # æ–°å¢ï¼šåˆå§‹æ•°æ®é›†è·¯å¾„
                 log_level: int = 1,  # æ–°å¢æ—¥å¿—çº§åˆ«å‚æ•°: 0=é™é»˜,1=æ­£å¸¸,2=è¯¦ç»†
                 ):
        
        # HFSSé…ç½®
        self.project_path = project_path
        self.design_name = design_name
        self.setup_name = setup_name
        self.sweep_name = sweep_name
        
        # ä¼˜åŒ–å‚æ•°
        self.variables = variables or []
        self.freq_range = freq_range
        self.constraints = constraints or []
        self.global_port_map = global_port_map or {}
        self.output_dir = output_dir
        self.max_active_cycles = max_active_cycles
        self.init_sample_multiplier = init_sample_multiplier
        self.feature_freq_points = min(feature_freq_points, 50)
        self.n_select = n_select
        self.ei_balance = ei_balance
        self.log_level = log_level  # ä¿å­˜æ—¥å¿—çº§åˆ«
        
        # å†…éƒ¨çŠ¶æ€
        self.hfss = None
        self.active_cycle = 0
        self.best_loss = float('inf')
        self.port_name_map = {}
        self.start_time = None

        # åˆ›å»ºæ•°æ®é›†å®ä¾‹
        self.dataset = HfssDataset(
            variables=self.variables,
            freq_range=self.freq_range,
            n_freq_points=self.feature_freq_points
        )
        # æ·»åŠ ç«¯å£æ˜ å°„
        for sp_name, ports in self.global_port_map.items():
            self.dataset.add_port_mapping(sp_name, ports[0], ports[1])

        self.gpr = None
        self.gpr_trainer = None  # æ–°å¢ï¼šGPRè®­ç»ƒå™¨å®ä¾‹å¼•ç”¨
        self.X_scaler = StandardScaler()
        self.y_scaler = StandardScaler()
        
        self.loss_history = []        # å­˜å‚¨æ¯è½®è¯„ä¼°æ ·æœ¬çš„æœ€å°æŸå¤±
        self.mse_history = []         # å­˜å‚¨æ¯è½®GPRè®­ç»ƒåçš„è®­ç»ƒé›†MSE
        self.unique_samples = set()   # è¿½è¸ªå·²æ¢ç´¢çš„è®¾è®¡ç‚¹

        # æ–°å¢ï¼šåˆå§‹æ•°æ®é›†è·¯å¾„
        self.initial_dataset_path = initial_dataset_path
        self.dataset_version = 1.0
        # ä¿®æ”¹ç›®å½•åˆ›å»ºé€»è¾‘
        if initial_dataset_path and os.path.exists(initial_dataset_path):
            # ä½¿ç”¨æ•°æ®é›†æ‰€åœ¨ç›®å½•ä½œä¸ºå·¥ä½œç›®å½•
            self.save_dir = os.path.dirname(initial_dataset_path)
            print(f"ğŸ’¾ ä½¿ç”¨ç°æœ‰å·¥ä½œç›®å½•: {self.save_dir}")
        else:
            # åˆ›å»ºæ–°çš„æ—¶é—´æˆ³ç›®å½•
            timestamp = time.strftime("%Y%m%d-%H%M%S")
            self.save_dir = os.path.join(self.output_dir, f"ai_optim_{timestamp}")
            os.makedirs(self.save_dir, exist_ok=True)
            print(f"ğŸ’¾ åˆ›å»ºæ–°å·¥ä½œç›®å½•: {self.save_dir}")

        # ç”Ÿæˆç‰¹å¾åˆ—å
        self._generate_feature_columns()
        
        print(f"âœ… AIä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ | ç‰¹å¾ç»´åº¦: {len(self.feature_columns)}"
              f"{' | ä½¿ç”¨é¢„åŠ è½½æ•°æ®é›†' if initial_dataset_path else ''}")

    def _generate_feature_columns(self):
        """ç”Ÿæˆæ ‡å‡†åŒ–ç‰¹å¾åˆ—å"""
        self.feature_columns = []
        
        # ç”Ÿæˆç›®æ ‡é¢‘ç‡ç‚¹
        target_freqs = np.linspace(
            self.freq_range[0] / 1e9, 
            self.freq_range[1] / 1e9, 
            self.feature_freq_points
        )
        
        # ä¸ºæ¯ä¸ªSå‚æ•°å’Œé¢‘ç‡ç‚¹åˆ›å»ºå®éƒ¨/è™šéƒ¨åˆ—
        for sp_name in self.global_port_map.keys():
            for freq in target_freqs:
                self.feature_columns.append(f"{sp_name}_{freq:.2f}GHz_real")
                self.feature_columns.append(f"{sp_name}_{freq:.2f}GHz_imag")

    # ======================== æ ¸å¿ƒä¼˜åŒ–æµç¨‹ ========================
    def optimize(self):
        """è¿è¡ŒAIä»£ç†ä¼˜åŒ–æµç¨‹"""
        print(f"\n{'='*60}")
        print("ğŸš€ å¯åŠ¨ HFSS AIä»£ç†ä¼˜åŒ–")
        print(f"ä¼˜åŒ–å˜é‡: {[v['name'] for v in self.variables]}")
        print(f"çº¦æŸæ¡ä»¶: {[c['expression'] for c in self.constraints]}")
        print(f"æœ€å¤§è½®æ¬¡: {self.max_active_cycles}")
        print('='*60)
        
        self.start_time = time.time()
        
        try:
            # åˆå§‹åŒ–HFSSç¯å¢ƒ
            self._initialize_hfss_environment()
            
            # ===== æ–°å¢ï¼šåˆå§‹æ•°æ®é›†åŠ è½½é€»è¾‘ =====
            if self.initial_dataset_path:
                print("\n" + "="*60)
                print(f"ğŸ” åŠ è½½åˆå§‹æ•°æ®é›†: {self.initial_dataset_path}")
                print('='*60)
                
                if not self.load_dataset(self.initial_dataset_path):
                    print("âŒ æ•°æ®é›†åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åˆå§‹é‡‡æ ·")
                    initial_samples = self.generate_initial_samples()
                    self.evaluate_samples(initial_samples, "Initial")
                    # æ–°æ•°æ®é›†æ‰éœ€è¦ä¿å­˜
                    self.save_dataset()  
                else:
                    print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›† | æ ·æœ¬æ•°: {self.dataset.size()}")

                    # éªŒè¯æ•°æ®é›†
                    if self.verify_dataset(self.initial_dataset_path):
                        print("âœ… æ•°æ®é›†éªŒè¯é€šè¿‡")
                    else:
                        print("âŒ æ•°æ®é›†éªŒè¯å¤±è´¥ï¼Œå†…å®¹ä¸ä¸€è‡´")
                    
                    # æ˜¾ç¤ºæ•°æ®é›†æ‘˜è¦
                    self.show_dataset_summary()
                    
                    # å¯¼å‡ºä¸ºCSV
                    csv_path = os.path.join(self.save_dir, "dataset_export.csv")
                    self.export_dataset_to_csv(csv_path)
                    # ä½¿ç”¨å·²æœ‰æ•°æ®é›†æ—¶ä¸é‡å¤ä¿å­˜
            else:
                # ç”Ÿæˆåˆå§‹æ ·æœ¬
                print("\n" + "="*60)
                print("ğŸŒŸ å¼€å§‹åˆå§‹æ ·æœ¬é‡‡æ ·")
                print('='*60)
                initial_samples = self.generate_initial_samples()
                self.evaluate_samples(initial_samples, "Initial")
                # æ–°æ•°æ®é›†éœ€è¦ä¿å­˜
                self.save_dataset() 
            
            # è®­ç»ƒGPRæ¨¡å‹
            print("\n" + "="*60)
            print("ğŸ§  è®­ç»ƒGPRä»£ç†æ¨¡å‹")
            print('='*60)
            # è®­ç»ƒGPRæ¨¡å‹
            if not self.train_feature_model():
                print("âŒ GPRæ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œç»ˆæ­¢ä¼˜åŒ–")
            else:
                # +++ æ–°å¢ï¼šè®­ç»ƒåç«‹å³éªŒè¯æ¨¡å‹ +++
                self.validate_model(n_samples=5)

            # ä¸»åŠ¨å­¦ä¹ ç­–ç•¥
            # +++ æ–°å¢ï¼šæ‰§è¡Œä¸»åŠ¨å­¦ä¹ å¾ªç¯ +++
            self.active_learning_cycle()
            
            # +++ æ–°å¢ï¼šæœ€ä¼˜è§£æå– +++
            self.extract_optimal_solutions()
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–å¤±è´¥: {str(e)}")
            traceback.print_exc()
        finally:
            # å¯¼å‡ºä¸ºCSV
            if self.dataset:
                csv_path = os.path.join(self.save_dir, "dataset_export.csv")
                self.export_dataset_to_csv(csv_path)
            # ç¡®ä¿å…³é—­HFSSè¿æ¥
            if self.hfss:
                
                self.hfss.close()

    def _initialize_hfss_environment(self):
        """åˆå§‹åŒ–HFSSç¯å¢ƒ"""
        self.hfss = HFSSController(
            project_path=self.project_path,
            design_name=self.design_name,
            setup_name=self.setup_name,
            sweep_name=self.sweep_name
        )
        
        if not self.hfss.connect():
            raise RuntimeError("HFSSè¿æ¥å¤±è´¥")
        
        if not self.hfss.check_design_config():
            raise RuntimeError("è®¾è®¡é…ç½®æ£€æŸ¥å¤±è´¥")
        
        # æ„å»ºç«¯å£åç§°æ˜ å°„
        self.build_port_name_map()

    def build_port_name_map(self):
        """æ„å»ºç«¯å£åç§°æ˜ å°„ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
        ports = self.hfss.get_ports()
        print(f"ğŸ”ŒğŸ”ŒğŸ”ŒğŸ”Œ æ£€æµ‹åˆ°çš„ç«¯å£: {ports}")
        
        self.port_name_map = {}
        for i, port in enumerate(ports, 1):
            # ä¿ç•™åŸå§‹ç«¯å£åç§°ï¼Œåªç§»é™¤ç©ºæ ¼ï¼ˆå†’å·æ˜¯æœ‰æ•ˆå­—ç¬¦ï¼‰
            clean_port = port.strip().replace(" ", "")  # ä»…ç§»é™¤ç©ºæ ¼
            self.port_name_map[str(i)] = clean_port
        
        # æ›´æ–°å…¨å±€ç«¯å£æ˜ å°„ï¼ˆä½¿ç”¨åŸå§‹åç§°ï¼‰
        for sp_name, (tx_id, rx_id) in self.global_port_map.items():
            tx_port = self.port_name_map.get(tx_id, tx_id)
            rx_port = self.port_name_map.get(rx_id, rx_id)
            # ç¡®ä¿ç«¯å£åç§°æ ¼å¼ä¸º "ç«¯å£å·:ç«¯å£å·"
            self.global_port_map[sp_name] = (tx_port, rx_port)
        
        print(f"ğŸ”€ğŸ”€ğŸ”€ğŸ”€ æ›´æ–°å…¨å±€ç«¯å£æ˜ å°„: {self.global_port_map}")

    # ======================== é‡‡æ ·æ–¹æ³• ========================
    def generate_initial_samples(self) -> np.ndarray:
        """ç”Ÿæˆæ‹‰ä¸è¶…ç«‹æ–¹åˆå§‹æ ·æœ¬"""
        n_vars = len(self.variables)
        n_samples = max(5, n_vars * self.init_sample_multiplier)
        
        print(f"ğŸ“Š ç”Ÿæˆåˆå§‹æ ·æœ¬: {n_samples}ä¸ªç‚¹")
        
        # ç”Ÿæˆæ‹‰ä¸è¶…ç«‹æ–¹æ ·æœ¬
        lhs_samples = lhs(n_vars, samples=n_samples, criterion='maximin')
        
        # æ˜ å°„åˆ°å®é™…å‚æ•°èŒƒå›´
        samples = np.zeros_like(lhs_samples)
        for i, var in enumerate(self.variables):
            low, high = var['bounds']
            samples[:, i] = lhs_samples[:, i] * (high - low) + low
        
        return samples

    # æ·»åŠ æœ€ä¼˜è§£æå–æ–¹æ³•
    def extract_optimal_solutions(self, n_candidates=10000, top_k=3):
        """æå–æœ€ä¼˜è§£ï¼ˆç‹¬ç«‹äºä¸»åŠ¨å­¦ä¹ ï¼‰"""
        print("\n" + "="*60)
        print("ğŸ†ğŸ† å¼€å§‹æœ€ä¼˜è§£æå–")
        print('='*60)
        
        # 1. é¢„ç­›é€‰
        print(f"ğŸ” ç”Ÿæˆ{n_candidates}ä¸ªå€™é€‰è§£...")
        candidates = self.generate_candidate_samples(n_candidates)
        
        print("ğŸ§  ä½¿ç”¨GPRé¢„æµ‹ç‰¹å¾...")
        y_pred, _ = self.gpr_trainer.predict(candidates)
        
        print("ğŸ“Š è®¡ç®—å€™é€‰è§£æŸå¤±...")
        losses = np.zeros(n_candidates)
        for i in range(n_candidates):
            losses[i] = self.calculate_potential_loss(y_pred[i])
        
        # ç­›é€‰Top-Kå€™é€‰è§£
        top_indices = np.argsort(losses)[:top_k]
        top_candidates = candidates[top_indices]
        top_losses = losses[top_indices]
        
        print(f"âœ… ç­›é€‰å‡ºTop-{top_k}å€™é€‰è§£ï¼ŒæŸå¤±èŒƒå›´: [{top_losses.min():.4f}, {top_losses.max():.4f}]")
        
        # 2. HFSSéªŒè¯
        print("\nğŸ”¬ å¼€å§‹HFSSéªŒè¯...")
        valid_solutions = []
        for i, candidate in enumerate(top_candidates):
            print(f"éªŒè¯å€™é€‰è§£ {i+1}/{len(top_candidates)} - é¢„æµ‹æŸå¤±: {top_losses[i]:.4f}")
            try:
                # è¯„ä¼°æ ·æœ¬
                self.evaluate_samples(np.array([candidate]), "Validation")
                
                # è·å–æœ€æ–°æ·»åŠ çš„æ ·æœ¬æŸå¤±
                actual_loss = self.calculate_latest_loss()
                
                # æ£€æŸ¥é¢„æµ‹è¯¯å·®
                pred_error = abs(actual_loss - top_losses[i])
                if pred_error < 0.1:  # 10%è¯¯å·®é˜ˆå€¼
                    valid_solutions.append({
                        "params": candidate,
                        "loss": actual_loss
                    })
                    print(f"âœ… éªŒè¯é€šè¿‡ | å®é™…æŸå¤±: {actual_loss:.4f} | è¯¯å·®: {pred_error:.4f}")
                else:
                    print(f"âš ï¸ é¢„æµ‹è¯¯å·®è¿‡å¤§ | å®é™…æŸå¤±: {actual_loss:.4f} | è¯¯å·®: {pred_error:.4f}")
                    
                # å¤±è´¥å¤„ç†ï¼šå°†æ ·æœ¬åŠ å…¥æ•°æ®é›†
                # æ— è®ºéªŒè¯æ˜¯å¦é€šè¿‡ï¼Œæ ·æœ¬å·²åŠ å…¥æ•°æ®é›†
                
            except Exception as e:
                print(f"âŒ éªŒè¯å¤±è´¥: {str(e)}")
        
        # 3. è¾“å‡ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ‰ğŸ‰ ä¼˜åŒ–ç»“æœæ‘˜è¦")
        print('='*60)
        if valid_solutions:
            # æŒ‰æŸå¤±æ’åº
            valid_solutions.sort(key=lambda x: x["loss"])
            best_solution = valid_solutions[0]
            print(f"ğŸ… æ‰¾åˆ° {len(valid_solutions)} ä¸ªæœ‰æ•ˆè§£")
            print(f"ğŸ¥‡ æœ€ä¼˜è§£æŸå¤±: {best_solution['loss']:.4f}")
            print("æœ€ä¼˜å‚æ•°:")
            for i, var in enumerate(self.variables):
                print(f"  {var['name']}: {best_solution['params'][i]:.4f} {var.get('unit', '')}")
            
            # ä¿å­˜ç»“æœ
            result_path = os.path.join(self.save_dir, "optimal_solutions.csv")
            self.save_solutions_to_csv(valid_solutions, result_path)
            return best_solution["params"]
        else:
            print("âŒ æœªæ‰¾åˆ°æ»¡è¶³è¦æ±‚çš„è§£")
            # å¤±è´¥å¤„ç†ï¼šé‡æ–°è®­ç»ƒæ¨¡å‹å¹¶é‡è¯•
            print("ğŸ”„ é‡æ–°è®­ç»ƒGPRæ¨¡å‹å¹¶é‡è¯•...")
            self.train_feature_model()
            return self.extract_optimal_solutions(n_candidates//2, top_k//2)  # å‡å°‘å€™é€‰è§„æ¨¡

    def calculate_latest_loss(self):
        """è®¡ç®—æœ€æ–°æ ·æœ¬çš„æŸå¤±"""
        if not self.dataset.X:
            return float('inf')
        
        # è·å–æœ€æ–°æ ·æœ¬çš„ç‰¹å¾
        latest_features = self.dataset.feature_vectors[-1]
        
        # é‡æ„Så‚æ•°ç‰¹å¾
        s_params_features = {}
        start_idx = 0
        for sp_name in self.global_port_map:
            n_points = len(self.dataset.target_freqs)
            sp_features = latest_features[start_idx:start_idx + n_points*2]
            s_params_features[sp_name] = sp_features.reshape(n_points, 2)
            start_idx += n_points*2
        
        # è®¡ç®—æŸå¤±
        return self.calculate_loss_from_features(s_params_features, verbose=0)  

    def save_solutions_to_csv(self, solutions, file_path):
        """ä¿å­˜æœ€ä¼˜è§£åˆ°CSV"""
        data = []
        for i, sol in enumerate(solutions):
            row = {"loss": sol["loss"], "rank": i+1}
            for j, var in enumerate(self.variables):
                row[var["name"]] = sol["params"][j]
            data.append(row)
        
        df = pd.DataFrame(data)
        df.to_csv(file_path, index=False)
        print(f"ğŸ’¾ æœ€ä¼˜è§£å·²ä¿å­˜è‡³: {file_path}")
        
    # ======================== ä»£ç†æ¨¡å‹ ========================
    def train_feature_model(self) -> bool:
        """è®­ç»ƒç‰¹å¾æ¨¡å‹(GPR)"""
        print("\nğŸ§  å¼€å§‹è®­ç»ƒGPRä»£ç†æ¨¡å‹...")
        
        # ç¡®ä¿æ•°æ®é›†å·²ä¿å­˜
        dataset_path = os.path.join(self.save_dir, "dataset.npz")
        if not self.initial_dataset_path:
            self.save_dataset()
        
        try:
            # åˆ›å»ºå¹¶è¿è¡ŒGPRè®­ç»ƒå™¨
            trainer = GPRTrainer(
                dataset_path=dataset_path,
                output_dir=self.save_dir
            )
            self.gpr_trainer = trainer  # å­˜å‚¨æ•´ä¸ªè®­ç»ƒå™¨
            self.gpr = trainer.run()
            
            # ä¿å­˜è®­ç»ƒå†å²
            self.save_training_history()

            return True
        except Exception as e:
            print(f"âŒ GPRè®­ç»ƒå¤±è´¥: {str(e)}")
            traceback.print_exc()
            return False
            
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = os.path.join(self.save_dir, "training_history.json")
        history = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "dataset_size": self.dataset.size(),
            "input_dim": len(self.variables),
            "output_dim": len(self.feature_columns),
            "gpr_kernel": str(self.gpr.kernel_),
            "log_likelihood": self.gpr.log_marginal_likelihood()
        }
        
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
            
        print(f"ğŸ“ è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {history_path}")

    # æ–°å¢ï¼šæ•°æ®é›†åŠ è½½æ–¹æ³•
    def load_dataset(self, file_path: str) -> bool:
        """ä»æ–‡ä»¶åŠ è½½æ•°æ®é›†"""
        try:
            if not os.path.exists(file_path):
                print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
                
            data = np.load(file_path, allow_pickle=True)
            
            # æ£€æŸ¥æ•°æ®é›†å…¼å®¹æ€§
            if 'version' not in data:
                print("âš ï¸ æ•°æ®é›†æ— ç‰ˆæœ¬ä¿¡æ¯ï¼Œå¯èƒ½ä¸å…¼å®¹")
                
            # é‡ç½®æ•°æ®é›†
            self.dataset = HfssDataset(
                variables=self.variables,
                freq_range=self.freq_range,
                n_freq_points=self.feature_freq_points
            )
            
            # æ·»åŠ ç«¯å£æ˜ å°„
            for sp_name, ports in self.global_port_map.items():
                self.dataset.add_port_mapping(sp_name, ports[0], ports[1])
            
            # åŠ è½½æ•°æ®
            self.dataset.X = data['X'].tolist()
            self.dataset.y = data['y'].tolist()
            self.dataset.feature_vectors = data['feature_vectors'].tolist()
            
            # æ¢å¤ç›®æ ‡é¢‘ç‡ç‚¹
            if 'target_freqs' in data:
                self.dataset.target_freqs = data['target_freqs']
            
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}")
            traceback.print_exc()
            return False
    # ======================== ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ ========================
    """
    ä¸»åŠ¨å­¦ä¹ å¾ªç¯ï¼š
    1. ç”Ÿæˆå€™é€‰æ± 
    2. GPRé¢„æµ‹ä¸ä¸ç¡®å®šæ€§è¯„ä¼°
    3. ä¸»åŠ¨å­¦ä¹ æ ·æœ¬ç­›é€‰
    4. HFSSéªŒè¯ä¸æ•°æ®é›†æ›´æ–°
    5. æ¨¡å‹é‡è®­ç»ƒ
    6. æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
    """
    def active_learning_cycle(self):
        """æ‰§è¡Œä¸»åŠ¨å­¦ä¹ å¾ªç¯"""
        print("\n" + "="*60)
        print("ğŸ”„ğŸ”„ è¿›å…¥ä¸»åŠ¨å­¦ä¹ å¾ªç¯")
        print('='*60)
        # ä¿å­˜å½“å‰æ—¥å¿—çº§åˆ«
        original_log_level = self.log_level

        try:
            # ä¸»åŠ¨å­¦ä¹ é˜¶æ®µä¸´æ—¶é™é»˜
            self.log_level = 0
            # ä¸»åŠ¨å­¦ä¹ å¾ªç¯
            for cycle in range(1, self.max_active_cycles + 1):
                print(f"\nğŸ” ä¸»åŠ¨å­¦ä¹ è½®æ¬¡ {cycle}/{self.max_active_cycles}")
                
                # 1. ç”Ÿæˆå€™é€‰æ± 
                candidate_samples = self.generate_candidate_samples(n_samples=1000)
                print(f"ğŸ“Š ç”Ÿæˆå€™é€‰æ± : {len(candidate_samples)}ä¸ªæ ·æœ¬")
                
                # 2. GPRé¢„æµ‹ä¸ä¸ç¡®å®šæ€§è¯„ä¼°
                print("ğŸ§  è¿›è¡ŒGPRé¢„æµ‹...")
                y_pred, y_std = self.gpr_trainer.predict(candidate_samples)
                print(f"ğŸ” é¢„æµ‹ç»“æœ: {y_pred}",f"é¢„æµ‹ä¸ç¡®å®šæ€§: {y_std}")
                
                # 3. ä¸»åŠ¨å­¦ä¹ æ ·æœ¬ç­›é€‰
                selected_indices = self.select_samples(candidate_samples, y_pred, y_std)
                selected_samples = candidate_samples[selected_indices]
                print(f"ğŸ” ç­›é€‰å‡º{len(selected_samples)}ä¸ªæ ·æœ¬è¿›è¡Œä»¿çœŸ")

                self.log_level = original_log_level
                
                # 4. HFSSéªŒè¯ä¸æ•°æ®é›†æ›´æ–°
                current_cycle_losses = self.evaluate_samples(selected_samples, f"Cycle_{cycle}")

                # å†æ¬¡é™é»˜è¿›è¡Œæ¨¡å‹æ›´æ–°
                self.log_level = 0

                # 5. æ¨¡å‹é‡è®­ç»ƒ
                print("ğŸ”„ æ›´æ–°GPRæ¨¡å‹...")
                self.train_feature_model()
                self.save_dataset()
                # 6. æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
                # åœ¨è¯„ä¼°æ ·æœ¬åè®°å½•æŸå¤±
                min_loss = min(current_cycle_losses)  # æœ¬è½®è¯„ä¼°çš„æœ€å°æŸå¤±
                self.loss_history.append(min_loss)
                print(f"ğŸ” æœ¬è½®è¯„ä¼°æœ€å°æŸå¤±: {min_loss}")
                # åœ¨è®­ç»ƒåè®°å½•MSE - ä¿®å¤è®¿é—®æ–¹å¼
                mse = self.gpr_trainer.get_train_mse()  # ç°åœ¨å¯ä»¥æ­£ç¡®è®¿é—®
                self.mse_history.append(mse)
                
                # æ£€æŸ¥æ”¶æ•›
                if self.check_convergence(cycle):
                    print("ğŸ¯ æ»¡è¶³æ”¶æ•›æ¡ä»¶ï¼Œæå‰ç»ˆæ­¢ä¸»åŠ¨å­¦ä¹ ")
                    break
        finally:
            # æ¢å¤æ—¥å¿—çº§åˆ«
            self.log_level = original_log_level
        print("\nâœ…âœ… ä¸»åŠ¨å­¦ä¹ å¾ªç¯å®Œæˆ")

    def generate_candidate_samples(self, n_samples=1000):
        """ç”Ÿæˆå€™é€‰æ ·æœ¬ç‚¹ï¼ˆå¸¦è¾¹ç•Œçº¦æŸï¼‰"""
        n_vars = len(self.variables)
        samples = np.random.uniform(size=(n_samples, n_vars))
        
        # æ˜ å°„åˆ°å®é™…å‚æ•°èŒƒå›´
        for i, var in enumerate(self.variables):
            low, high = var['bounds']
            samples[:, i] = samples[:, i] * (high - low) + low
        
        return samples

    def select_samples(self, candidates, y_pred, y_std):
        """æ ¹æ®æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ç­–ç•¥ç­›é€‰æ ·æœ¬"""
        # 1. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ½œåœ¨æ€§èƒ½ï¼ˆPï¼‰
        losses = np.array([self.calculate_potential_loss(y) for y in y_pred])
        
        # 2. å½’ä¸€åŒ–æ€§èƒ½æ½œåŠ›ï¼ˆPï¼‰å’Œä¸ç¡®å®šæ€§ï¼ˆÏƒï¼‰
        P = -losses  # æŸå¤±è¶Šå°ï¼Œæ€§èƒ½æ½œåŠ›è¶Šå¤§
        Ïƒ = np.max(y_std, axis=1)  # å–æœ€å¤§æ ‡å‡†å·®ä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡
        
        P_norm = (P - P.min()) / (P.max() - P.min() + 1e-8)
        Ïƒ_norm = Ïƒ / (Ïƒ.max() + 1e-8)
        
        # 3. è®¡ç®—ç»¼åˆè¯„åˆ†
        scores = self.ei_balance * Ïƒ_norm + (1 - self.ei_balance) * P_norm
        
        # 4. é€‰æ‹©å¾—åˆ†æœ€é«˜çš„æ ·æœ¬
        top_indices = np.argsort(scores)[-self.n_select:]
        
        return top_indices

    def calculate_potential_loss(self, y_pred):
        """ä»é¢„æµ‹ç‰¹å¾å‘é‡è®¡ç®—æ½œåœ¨æŸå¤±"""
        # é‡æ„Så‚æ•°ç‰¹å¾
        s_params_features = {}
        start_idx = 0
        for sp_name in self.global_port_map:
            n_points = len(self.dataset.target_freqs)
            # æå–è¯¥Så‚æ•°çš„ç‰¹å¾éƒ¨åˆ†
            sp_features = y_pred[start_idx:start_idx + n_points*2]
            s_params_features[sp_name] = sp_features.reshape(n_points, 2)
            start_idx += n_points*2
        
        # è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨ä¸çœŸå®è¯„ä¼°ç›¸åŒçš„çº¦æŸé€»è¾‘ï¼‰
        return self.calculate_loss_from_features(s_params_features)

    def calculate_loss_from_features(self, s_params_features, verbose=None):
        """ä»ç‰¹å¾å‘é‡è®¡ç®—æŸå¤±ï¼ˆæ›¿ä»£ä»¿çœŸï¼‰"""
        # åˆ›å»ºè™šæ‹Ÿçš„DataFrameç»“æ„
        freq_points = self.dataset.target_freqs
        s_params = pd.DataFrame({'Frequency': freq_points})
        
        # æ„å»ºå¤æ•°Så‚æ•° - ä½¿ç”¨ä¸HFSSä¸€è‡´çš„åˆ—åæ ¼å¼
        for sp_name, features in s_params_features.items():
            real_part = features[:, 0]
            imag_part = features[:, 1]
            s_complex = real_part + 1j * imag_part
            
            # è·å–å¯¹åº”çš„ç«¯å£ç»„åˆ
            tx, rx = self.global_port_map[sp_name]
            
            # åˆ›å»ºä¸HFSSä¸€è‡´çš„åˆ—åæ ¼å¼
            col_name = f"S({tx},{rx})"
            
            # æ·»åŠ åˆ°DataFrame
            s_params[col_name] = s_complex
        
        # è®¡ç®—æŸå¤±
        return self.calculate_loss(s_params, self.constraints, verbose=verbose)

    def check_convergence(self, cycle):
        """åŸºäºå®é™…è¯„ä¼°ç»“æœçš„æ”¶æ•›åˆ¤æ–­"""
        if cycle < 3:  # è‡³å°‘è¿è¡Œ5è½®
            return False
            
        # 1. æŸå¤±ç¨³å®šæ€§æ£€æŸ¥ (æœ€è¿‘3è½®æŸå¤±å˜åŒ–<1%)
        if len(self.loss_history) >= 3:
            recent_losses = self.loss_history[-3:]
            loss_change = abs(max(recent_losses) - min(recent_losses)) / max(recent_losses)
            loss_stable = loss_change < 0.0001
        else:
            loss_stable = False
            
        # 2. æ¨¡å‹æ€§èƒ½é¥±å’Œæ£€æŸ¥ (MSEå˜åŒ–<2%)
        if len(self.mse_history) >= 3:
            recent_mse = self.mse_history[-3:]
            mse_change = abs(max(recent_mse) - min(recent_mse)) / min(recent_mse)
            mse_saturated = mse_change < 0.02
        else:
            mse_saturated = False
            
        # 3. è®¾è®¡ç©ºé—´æ¢ç´¢æ£€æŸ¥ (å·²æ¢ç´¢åŒºåŸŸæ¯”ä¾‹)
        n_unique = len(self.unique_samples)
        exploration_ratio = n_unique / (cycle * self.n_select)
        well_explored = exploration_ratio > 0.8  # 80%çš„è®¾è®¡ç‚¹æ˜¯æ–°çš„
        
        # ç»¼åˆæ”¶æ•›æ¡ä»¶
        convergence_reached = loss_stable and mse_saturated
        
        # æ‰“å°æ”¶æ•›è¯Šæ–­ä¿¡æ¯
        print("\nğŸ”„ æ”¶æ•›è¯Šæ–­:")
        print(f"  - æŸå¤±ç¨³å®šæ€§: {'ç¨³å®š' if loss_stable else 'ä¸ç¨³å®š'} (å˜åŒ–: {loss_change*100:.2f}%)")
        print(f"  - æ¨¡å‹æ€§èƒ½: {'é¥±å’Œ' if mse_saturated else 'æå‡ä¸­'} (MSEå˜åŒ–: {mse_change*100:.2f}%)")
        print(f"  - ç©ºé—´æ¢ç´¢: {exploration_ratio*100:.1f}% æ–°è®¾è®¡ç‚¹")
        
        return convergence_reached

    # ======================== è¯„ä¼°ä¸æ›´æ–° ========================
    def evaluate_samples(self, samples: np.ndarray, eval_type: str):
        """è¯„ä¼°æ ·æœ¬ç‚¹å¹¶æ›´æ–°æ•°æ®é›†
        å†…å®¹åŒ…å«ï¼šè·å–å¤æ•°Så‚æ•°ï¼›
                 è®¡ç®—æŸå¤±ï¼›
                 æå–ç‰¹å¾ï¼›
                 æ„å»ºæ•°æ®é›†ã€‚
        """
        print(f"\nğŸ”¬ è¯„ä¼° {len(samples)} ä¸ªæ ·æœ¬ | é˜¶æ®µ: {eval_type}")
            
        success_count = 0
        current_cycle_losses = []
        for i in range(samples.shape[0]):
            print(f"ğŸ“Š è¯„ä¼°æ ·æœ¬ {i+1}/{samples.shape[0]}")
            sample = samples[i]
            try:
                # è®¾ç½®å˜é‡
                for j, var in enumerate(self.variables):
                    value = sample[j]
                    self.hfss.set_variable(var['name'], value, unit=var.get('unit'))
                
                # è¿è¡Œä»¿çœŸ
                if not self.hfss.analyze():
                    raise RuntimeError("ä»¿çœŸå¤±è´¥")
                
                # è·å–Så‚æ•°
                port_combos = list(set(self.global_port_map.values()))
                s_params = self.hfss.get_s_params(
                    port_combinations=port_combos,
                    data_format="complex"
                )
                
                if s_params is None:
                    raise RuntimeError("è·å–Så‚æ•°å¤±è´¥")
                #print(s_params)

                loss = self.calculate_loss(s_params, constraints=self.constraints)
                current_cycle_losses.append(loss)
                print('loss:',loss)
                # è®°å½•è®¾è®¡ç‚¹
                sample_hash = hash(tuple(sample))
                self.unique_samples.add(sample_hash)

                # æå–ç‰¹å¾å¹¶æ·»åŠ åˆ°æ•°æ®é›†
                features = self.dataset.extract_complex_features(s_params)
                #print('features:',features)
                self.dataset.add_sample(sample, features)
                # è·å–å¹³é“ºæ•°æ®é›†
                #x_dataset, y_dataset = self.dataset.get_flat_dataset()
                #print('x_dataset:',x_dataset)
                #print('y_dataset:',y_dataset)
                success_count += 1
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬è¯„ä¼°å¤±è´¥: {str(e)}")
        
        print(f"âœ… æˆåŠŸè¯„ä¼°: {success_count}/{len(samples)} ä¸ªæ ·æœ¬")
        return current_cycle_losses

    def calculate_loss(self, s_params: pd.DataFrame, constraints: List[dict], verbose: int = None) -> float:
        """è®¡ç®—æŸå¤±å€¼ï¼Œæ”¯æŒå¤æ‚è¡¨è¾¾å¼å’Œèšåˆå‡½æ•°"""
        verbose = verbose if verbose is not None else self.log_level
        total_loss = 0.0
        
        # åˆ›å»ºå¸¦dBå’ŒåŸå§‹å¤æ•°çš„å‰¯æœ¬
        s_params_ext = s_params.copy()
        for column in s_params.columns:
            if column != 'Frequency' and column.startswith('S('):
                s_params_ext[f"{column}_dB"] = 20 * np.log10(np.abs(s_params[column]))
                s_params_ext[f"{column}_real"] = np.real(s_params[column])
                s_params_ext[f"{column}_imag"] = np.imag(s_params[column])
        
        if verbose >= 1:
            print("\nçº¦æŸè®¡ç®—ç»“æœ:")
            print("-"*60)
            print(f"{'çº¦æŸè¡¨è¾¾å¼':<30} | {'ç›®æ ‡å€¼':<14} | {'å®é™…å€¼':<14} | {'æŸå¤±è´¡çŒ®':<10}")
            print("-"*60)
        
        # é¢„å®šä¹‰æ”¯æŒçš„èšåˆå‡½æ•°
        AGG_FUNCTIONS = {
            'mean': np.mean,
            'max': np.max,
            'min': np.min,
            'sum': np.sum
        }
        
        for constraint in constraints:
            expr = constraint['expression']
            target = constraint['target']
            operator = constraint['operator']
            weight = constraint['weight']
            freq_range = constraint.get('freq_range')
            aggregate = constraint.get('aggregate', 'mean')  # é»˜è®¤ä½¿ç”¨å‡å€¼èšåˆ
            
            try:
                # 1. ç­›é€‰é¢‘ç‡èŒƒå›´
                if freq_range:
                    freq_min_ghz = freq_range[0] / 1e9
                    freq_max_ghz = freq_range[1] / 1e9
                    df_sub = s_params_ext[
                        (s_params_ext['Frequency'] >= freq_min_ghz) & 
                        (s_params_ext['Frequency'] <= freq_max_ghz)
                    ]
                else:
                    df_sub = s_params_ext
                    
                if len(df_sub) == 0:
                    print(f"âš ï¸ è­¦å‘Š: çº¦æŸ '{expr}' åœ¨é¢‘ç‡èŒƒå›´å†…æ— æ•°æ®ç‚¹")
                    loss = 10 * weight
                    total_loss += loss
                    if verbose >= 1:
                        print(f"{expr:<30} | {target:<14.4f} | {'N/A':<14} | {loss:<10.4f}")
                    continue
                    
                # 2. è§£æè¡¨è¾¾å¼ï¼ˆä½¿ç”¨èšåˆå‡½æ•°ï¼‰
                agg_func = AGG_FUNCTIONS.get(aggregate)
                actual_value = self._eval_expression(df_sub, expr, agg_func)
                
                # 3. ç¡®ä¿actual_valueæ˜¯æ ‡é‡
                if isinstance(actual_value, np.ndarray):
                    if actual_value.size == 1:
                        actual_value = actual_value.item()
                    else:
                        # å¯¹äºæ•°ç»„ï¼Œä½¿ç”¨æŒ‡å®šçš„èšåˆå‡½æ•°
                        actual_value = agg_func(actual_value) if agg_func else np.mean(actual_value)
                
                # 4. è®¡ç®—æŸå¤±
                if operator == '<':
                    violation = max(actual_value - target, 0)
                    loss = weight * (violation ** 2)  # å¹³æ–¹æŸå¤±
                elif operator == '>':
                    violation = max(target - actual_value, 0)
                    loss = weight * (violation ** 2)
                else:  # ç­‰å¼çº¦æŸ
                    loss = weight * abs(actual_value - target)
                    
                total_loss += loss
                
                if verbose >= 1:
                    print(f"{expr:<30} | {target:<14.4f} | {actual_value:<14.4f} | {loss:<10.4f}")
                        
            except Exception as e:
                print(f"âŒâŒ çº¦æŸè®¡ç®—å¤±è´¥: {expr} | é”™è¯¯: {str(e)}")
                traceback.print_exc()
                loss = 10 * weight
                total_loss += loss
                if verbose >= 1:
                    print(f"{expr:<30} | {target:<14.4f} | {'ERROR':<14} | {loss:<10.4f}")
        
        if verbose >= 1:
            print("-"*60)
            print(f"{'æ€»æŸå¤±':<30} | {'':<14} | {'':<14} | {total_loss:<10.4f}")
            print("-"*60)
        
        return total_loss

    def _eval_expression(self, df: pd.DataFrame, expr: str, agg_func=None):
        """è¡¨è¾¾å¼æ±‚å€¼å¼•æ“ - ä¿®å¤æ‹¬å·å¤„ç†é—®é¢˜"""
        # 0. å¤„ç†å¸¦æ‹¬å·çš„è¡¨è¾¾å¼ - ä½¿ç”¨æ ˆå®ç°æ‹¬å·åŒ¹é…
        if '(' in expr:
            stack = []
            start_index = expr.find('(')
            for i in range(len(expr)):
                if expr[i] == '(':
                    stack.append(i)
                elif expr[i] == ')':
                    if stack:
                        start = stack.pop()
                        if not stack:  # æ‰¾åˆ°æœ€å¤–å±‚åŒ¹é…çš„æ‹¬å·
                            inner_expr = expr[start+1:i]
                            prefix = expr[:start].strip()
                            suffix = expr[i+1:].strip()
                            
                            # é€’å½’è§£æå†…éƒ¨è¡¨è¾¾å¼
                            inner_value = self._eval_expression(df, inner_expr, agg_func)
                            
                            # å¦‚æœæœ‰å‡½æ•°åï¼Œåº”ç”¨å‡½æ•°
                            if prefix and prefix.lower() in ['db', 'real', 'imag']:
                                if prefix.lower() == 'db':
                                    inner_value = 20 * np.log10(np.abs(inner_value))
                                elif prefix.lower() == 'real':
                                    inner_value = np.real(inner_value)
                                elif prefix.lower() == 'imag':
                                    inner_value = np.imag(inner_value)
                            
                            # é€’å½’å¤„ç†åç¼€è¡¨è¾¾å¼
                            if suffix:
                                # æ„é€ æ–°è¡¨è¾¾å¼ï¼šå†…éƒ¨ç»“æœ + åç¼€
                                new_expr = f"{inner_value}{suffix}"
                                return self._eval_expression(df, new_expr, agg_func)
                            return inner_value
            # å¦‚æœæ‰€æœ‰æ‹¬å·éƒ½å¤„ç†å®Œï¼Œè¿”å›åŸå§‹è¡¨è¾¾å¼
            return self._eval_expression(df, expr.replace('(', '').replace(')', ''), agg_func)

        # 1. å¤„ç†åŸºæœ¬è¿ç®—
        if '+' in expr:
            parts = expr.split('+')
            return sum(self._eval_expression(df, p, agg_func) for p in parts)
        elif '-' in expr:
            parts = expr.split('-')
            return self._eval_expression(df, parts[0], agg_func) - sum(self._eval_expression(df, p, agg_func) for p in parts[1:])
        elif '*' in expr:
            parts = expr.split('*')
            return np.prod([self._eval_expression(df, p, agg_func) for p in parts])
        elif '/' in expr:
            parts = expr.split('/')
            val = self._eval_expression(df, parts[0], agg_func)
            for p in parts[1:]:
                val /= self._eval_expression(df, p, agg_func)
            return val

        # 2. å¤„ç†Så‚æ•°å¼•ç”¨
        if expr in self.global_port_map:
            port_name = expr
            tx, rx = self.global_port_map[port_name]
            col_name = f"S({tx},{rx})"
            
            if col_name in df.columns:
                values = df[col_name].values
                return agg_func(values) if agg_func else values
            else:
                # å°è¯•æ·»åŠ åç¼€
                for suffix in ['', '_dB', '_real', '_imag']:
                    full_col = col_name + suffix
                    if full_col in df.columns:
                        values = df[full_col].values
                        return agg_func(values) if agg_func else values

        # 3. ç›´æ¥å¼•ç”¨åˆ—å
        if expr in df.columns:
            values = df[expr].values
            return agg_func(values) if agg_func else values

        # 4. å°è¯•è§£æä¸ºæ•°å€¼
        try:
            return float(expr)
        except ValueError:
            # å°è¯•è¯†åˆ«Så‚æ•°å˜ä½“
            if expr.lower().startswith('s') and any(char.isdigit() for char in expr):
                port_name = expr.upper()
                if port_name in self.global_port_map:
                    tx, rx = self.global_port_map[port_name]
                    col_name = f"S({tx},{rx})"
                    if col_name in df.columns:
                        values = df[col_name].values
                        return agg_func(values) if agg_func else values
            
            # æœ€åå°è¯•æ‰€æœ‰å¯èƒ½çš„åˆ—åå˜ä½“
            possible_cols = [col for col in df.columns if expr.lower() in col.lower()]
            if possible_cols:
                values = df[possible_cols[0]].values
                return agg_func(values) if agg_func else values
            
            # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError(f"æ— æ³•è§£æè¡¨è¾¾å¼: {expr}")
        
    # ======================== è¾…åŠ©æ–¹æ³• ======================== 
    def save_dataset(self, filename='dataset.npz'):
        """åªåœ¨éœ€è¦æ—¶ä¿å­˜æ•°æ®é›†"""
        if not self.dataset.X:
            print("âš ï¸ æ•°æ®é›†ä¸ºç©ºï¼Œä¸ä¿å­˜")
            return
             
        # ä½¿ç”¨åˆå§‹æ•°æ®é›†è·¯å¾„æˆ–é»˜è®¤æ–‡ä»¶å
        save_path = self.initial_dataset_path or os.path.join(self.save_dir, filename)
        
        try:
            np.savez(save_path,
                    X=self.dataset.X,
                    y=self.dataset.y,
                    feature_vectors=self.dataset.feature_vectors,
                    variables=self.variables,
                    port_mappings=self.global_port_map,
                    target_freqs=self.dataset.target_freqs,
                    version=self.dataset_version)
            print(f"ğŸ’¾ æ•°æ®é›†å·²ä¿å­˜è‡³: {save_path}")
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®é›†ä¿å­˜å¤±è´¥: {str(e)}")
            return False

    def verify_dataset(self, file_path: str) -> bool:
        """éªŒè¯æ•°æ®é›†å†…å®¹æ˜¯å¦ä¸€è‡´"""
        if not os.path.exists(file_path):
            print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        
        try:
            # åŠ è½½å¾…éªŒè¯çš„æ•°æ®é›†
            data = np.load(file_path, allow_pickle=True)
            print(f"\nğŸ” æ­£åœ¨éªŒè¯æ•°æ®é›†: {file_path}")
            
            # æ£€æŸ¥åŸºæœ¬å±æ€§
            print(f"æ•°æ®é›†ç‰ˆæœ¬: {data.get('version', 'æœªçŸ¥')}")
            print(f"å˜é‡æ•°é‡: {len(data['variables'])}")
            print(f"æ ·æœ¬æ•°é‡: {len(data['X'])}")
            
            # æ£€æŸ¥å˜é‡ä¸€è‡´æ€§
            current_vars = sorted([v['name'] for v in self.variables])
            loaded_vars = sorted([v['name'] for v in data['variables']])
            if current_vars != loaded_vars:
                print(f"âŒ å˜é‡ä¸åŒ¹é… | å½“å‰: {current_vars} | åŠ è½½: {loaded_vars}")
                return False
            else:
                print(f"âœ… å˜é‡åŒ¹é…: {current_vars}")
            
            # æ£€æŸ¥ç«¯å£æ˜ å°„
            current_ports = sorted(self.global_port_map.keys())
            loaded_ports = sorted(data['port_mappings'].item().keys())
            if current_ports != loaded_ports:
                print(f"âŒ ç«¯å£æ˜ å°„ä¸åŒ¹é… | å½“å‰: {current_ports} | åŠ è½½: {loaded_ports}")
                return False
            else:
                print(f"âœ… ç«¯å£æ˜ å°„åŒ¹é…: {current_ports}")
                
            # æ£€æŸ¥é¢‘ç‡èŒƒå›´
            current_freqs = self.dataset.target_freqs
            loaded_freqs = data['target_freqs']
            if not np.allclose(current_freqs, loaded_freqs):
                print(f"âŒ é¢‘ç‡ç‚¹ä¸åŒ¹é… | å½“å‰: {current_freqs[:5]}... | åŠ è½½: {loaded_freqs[:5]}...")
                return False
            else:
                print(f"âœ… é¢‘ç‡ç‚¹åŒ¹é…")
            
            # æ£€æŸ¥æ ·æœ¬æ•°æ®å®Œæ•´æ€§
            for i in range(min(3, len(data['X']))):  # æ£€æŸ¥å‰3ä¸ªæ ·æœ¬
                print(f"\næ ·æœ¬ #{i+1} éªŒè¯:")
                # æ£€æŸ¥è¾“å…¥å‚æ•°
                if not np.allclose(data['X'][i], self.dataset.X[i]):
                    print(f"âŒ è¾“å…¥å‚æ•°ä¸åŒ¹é…")
                    return False
                else:
                    print(f"âœ… è¾“å…¥å‚æ•°åŒ¹é…")
                    
                # æ£€æŸ¥ç‰¹å¾å‘é‡
                if not np.allclose(data['feature_vectors'][i], self.dataset.feature_vectors[i]):
                    print(f"âŒ ç‰¹å¾å‘é‡ä¸åŒ¹é…")
                    return False
                else:
                    print(f"âœ… ç‰¹å¾å‘é‡åŒ¹é…")
                    
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®é›†éªŒè¯å¤±è´¥: {str(e)}")
            traceback.print_exc()
            return False

    def show_dataset_summary(self):
        """æ˜¾ç¤ºæ•°æ®é›†æ‘˜è¦ä¿¡æ¯"""
        if not self.dataset.X:
            print("âš ï¸ æ•°æ®é›†ä¸ºç©º")
            return
        
        print("\nğŸ“Š æ•°æ®é›†æ‘˜è¦:")
        print("-"*60)
        print(f"æ ·æœ¬æ•°é‡: {len(self.dataset.X)}")
        print(f"è¾“å…¥ç»´åº¦: {len(self.variables)}")
        print(f"ç‰¹å¾ç»´åº¦: {len(self.dataset.feature_vectors[0])}")
        
        # æ˜¾ç¤ºå˜é‡èŒƒå›´
        print("\nå˜é‡èŒƒå›´:")
        for i, var in enumerate(self.variables):
            values = [sample[i] for sample in self.dataset.X]
            print(f"{var['name']}: min={min(values):.4f}, max={max(values):.4f}")
        
        # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
        features = np.array(self.dataset.feature_vectors)
        real_parts = features[:, ::2]  # æ‰€æœ‰å®éƒ¨
        imag_parts = features[:, 1::2]  # æ‰€æœ‰è™šéƒ¨
        
        print("\nç‰¹å¾ç»Ÿè®¡:")
        print(f"å®éƒ¨å‡å€¼: {np.mean(real_parts):.4f} Â± {np.std(real_parts):.4f}")
        print(f"è™šéƒ¨å‡å€¼: {np.mean(imag_parts):.4f} Â± {np.std(imag_parts):.4f}")
        print(f"ç‰¹å¾å€¼èŒƒå›´: [{np.min(features):.4f}, {np.max(features):.4f}]")
        
        # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬
        print("\nå‰3ä¸ªæ ·æœ¬ç¤ºä¾‹:")
        for i in range(min(3, len(self.dataset.X))):
            print(f"\næ ·æœ¬ #{i+1}:")
            # è¾“å…¥å‚æ•°
            params = [f"{var['name']}={val:.4f}" 
                    for var, val in zip(self.variables, self.dataset.X[i])]
            print(f"å‚æ•°: {', '.join(params)}")
            
            # ç‰¹å¾å‘é‡(ç®€åŒ–æ˜¾ç¤º)
            features = self.dataset.feature_vectors[i]
            print(f"ç‰¹å¾: [å®éƒ¨: {features[0]:.4f}, è™šéƒ¨: {features[1]:.4f}, ...] "
                f"(å…±{len(features)}ä¸ªå€¼)")
        
        print("-"*60)

    def export_dataset_to_csv(self, file_path: str):
        """å°†æ•°æ®é›†å¯¼å‡ºä¸ºCSVæ–‡ä»¶"""
        try:
            if not self.dataset.X:
                print("âš ï¸ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•å¯¼å‡º")
                return False
                
            # åˆ›å»ºDataFrame
            data = []
            for i, (x, features) in enumerate(zip(self.dataset.X, self.dataset.feature_vectors)):
                row = {f"param_{j}": val for j, val in enumerate(x)}
                row.update({f"feat_{j}": val for j, val in enumerate(features)})
                data.append(row)
                
            df = pd.DataFrame(data)
            
            # æ·»åŠ åˆ—åæ˜ å°„
            param_names = [var['name'] for var in self.variables]
            for i, name in enumerate(param_names):
                df = df.rename(columns={f"param_{i}": name})
                
            # å¯¼å‡ºCSV
            df.to_csv(file_path, index=False)
            print(f"ğŸ’¾ æ•°æ®é›†å·²å¯¼å‡ºä¸ºCSV: {file_path}")
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®é›†å¯¼å‡ºå¤±è´¥: {str(e)}")
            return False

    def complex_to_dB(self, complex_values):
        """å°†å¤æ•°Så‚æ•°è½¬æ¢ä¸ºdBæ ¼å¼"""
        return 20 * np.log10(np.abs(complex_values))
    
    def validate_model(self, n_samples=10):
        """éªŒè¯æ¨¡å‹æ€§èƒ½ï¼šå¯¹æ¯”é¢„æµ‹ç»“æœä¸çœŸå®å€¼"""
        if self.log_level < 1:
            return
            
        print("\n" + "="*60)
        print("ğŸ§ªğŸ§ª å¼€å§‹æ¨¡å‹éªŒè¯")
        print('='*60)
        
        # 1. é€‰æ‹©éªŒè¯æ ·æœ¬
        dataset_size = self.dataset.size()
        if dataset_size == 0:
            print("âš ï¸ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•éªŒè¯")
            return
            
        # éšæœºé€‰æ‹©æ ·æœ¬ç´¢å¼•
        sample_indices = np.random.choice(dataset_size, min(n_samples, dataset_size), replace=False)
        
        # 2. å‡†å¤‡æ•°æ®å®¹å™¨
        all_results = []
        
        # 3. å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡ŒéªŒè¯
        for idx in sample_indices:
            sample_params = np.array(self.dataset.X[idx]).reshape(1, -1)
            true_features = self.dataset.feature_vectors[idx]
            
            # æ¨¡å‹é¢„æµ‹
            pred_features, _ = self.gpr_trainer.predict(sample_params)
            pred_features = pred_features[0]  # å»æ‰batchç»´åº¦
            
            # é‡æ„Så‚æ•°
            true_s_params = self.reconstruct_s_params(true_features)
            pred_s_params = self.reconstruct_s_params(pred_features)
            
            # è½¬æ¢ä¸ºdBæ ¼å¼
            true_s_params_dB = {k: self.complex_to_dB(v) for k, v in true_s_params.items()}
            pred_s_params_dB = {k: self.complex_to_dB(v) for k, v in pred_s_params.items()}
            
            # 4. å­˜å‚¨ç»“æœ
            sample_results = {
                'index': idx,
                'params': sample_params[0].tolist(),
                'true': true_s_params_dB,
                'pred': pred_s_params_dB
            }
            all_results.append(sample_results)
            
            # 5. å¯è§†åŒ–å¯¹æ¯”
            self.plot_comparison(idx, true_s_params_dB, pred_s_params_dB)
        
        # 6. ä¿å­˜CSVç»“æœ
        self.save_validation_csv(all_results)
        print("âœ…âœ… æ¨¡å‹éªŒè¯å®Œæˆ")
    
    def reconstruct_s_params(self, feature_vector):
        """ä»ç‰¹å¾å‘é‡é‡æ„Så‚æ•°"""
        s_params = {}
        start_idx = 0
        for sp_name in self.global_port_map:
            n_points = len(self.dataset.target_freqs)
            # æå–è¯¥Så‚æ•°çš„ç‰¹å¾éƒ¨åˆ†å¹¶è½¬æ¢ä¸ºNumPyæ•°ç»„
            sp_features = np.array(feature_vector[start_idx:start_idx + n_points*2])
            
            # ä½¿ç”¨NumPyæ­£ç¡®åˆ›å»ºå¤æ•°æ•°ç»„
            real_arr = sp_features[::2]  # æ‰€æœ‰å®éƒ¨
            imag_arr = sp_features[1::2]  # æ‰€æœ‰è™šéƒ¨
            s_complex = real_arr.astype(complex)  # è½¬æ¢ä¸ºå¤æ•°æ•°ç»„
            s_complex.imag = imag_arr  # è®¾ç½®è™šéƒ¨
            
            s_params[sp_name] = s_complex
            start_idx += n_points*2
        return s_params
    
    def plot_comparison(self, idx, true_dB, pred_dB):
        """ç»˜åˆ¶é¢„æµ‹ä¸çœŸå®å€¼çš„å¯¹æ¯”å›¾"""
        n_ports = len(true_dB)
        fig, axs = plt.subplots(n_ports, 1, figsize=(10, 4*n_ports))
        fig.suptitle(f"æ ·æœ¬ #{idx} é¢„æµ‹ä¸çœŸå®å€¼å¯¹æ¯”", fontsize=16)
        
        # è·å–ç›®æ ‡é¢‘ç‡ç‚¹
        freqs = self.dataset.target_freqs
        
        for i, (sp_name, true_values) in enumerate(true_dB.items()):
            ax = axs[i] if n_ports > 1 else axs
            pred_values = pred_dB[sp_name]
            
            # ç»˜åˆ¶æ›²çº¿
            ax.plot(freqs, true_values, 'b-', label='çœŸå®å€¼', linewidth=2)
            ax.plot(freqs, pred_values, 'r--', label='é¢„æµ‹å€¼', linewidth=1.5)
            
            # è®¡ç®—è¯¯å·®
            errors = np.abs(true_values - pred_values)
            max_error = np.max(errors)
            avg_error = np.mean(errors)
            
            # æ·»åŠ æ ‡æ³¨
            ax.set_title(f"{sp_name} | æœ€å¤§è¯¯å·®: {max_error:.2f}dB, å¹³å‡è¯¯å·®: {avg_error:.2f}dB")
            ax.set_xlabel('é¢‘ç‡ (GHz)')
            ax.set_ylabel('å¹…åº¦ (dB)')
            ax.legend()
            ax.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        plot_path = os.path.join(self.save_dir, f"validation_sample_{idx}.png")
        plt.savefig(plot_path)
        plt.close()
        print(f"ğŸ“Š éªŒè¯å¯¹æ¯”å›¾å·²ä¿å­˜: {plot_path}")
    
    def save_validation_csv(self, all_results):
        """å°†éªŒè¯ç»“æœå¯¼å‡ºä¸ºCSV"""
        csv_data = []
        
        # ç”Ÿæˆé¢‘ç‡åˆ—å
        freqs = [f"{freq:.2f}GHz" for freq in self.dataset.target_freqs]
        
        for result in all_results:
            for sp_name, true_values in result['true'].items():
                pred_values = result['pred'][sp_name]
                
                for freq, t_val, p_val in zip(freqs, true_values, pred_values):
                    row = {
                        'sample_index': result['index'],
                        'sp_param': sp_name,
                        'frequency': freq,
                        'true_dB': t_val,
                        'pred_dB': p_val,
                        'error': abs(t_val - p_val)
                    }
                    # æ·»åŠ å‚æ•°å€¼
                    for i, var in enumerate(self.variables):
                        row[var['name']] = result['params'][i]
                    
                    csv_data.append(row)
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(csv_data)
        csv_path = os.path.join(self.save_dir, "model_validation_results.csv")
        df.to_csv(csv_path, index=False)
        print(f"ğŸ’¾ éªŒè¯ç»“æœCSVå·²ä¿å­˜: {csv_path}")

# ä¸»å‡½æ•°ç¤ºä¾‹
def main():
    """ä¸»å‡½æ•° - ä¼˜åŒ–ç¤ºä¾‹"""
    # é¡¹ç›®é…ç½®
    PROJECT_PATH = r"C:\Users\Administrator\Desktop\huaSheng\6G\6G.aedt"
    DESIGN_NAME = "HFSSDesign5"
    SETUP_NAME = "Setup1"
    SWEEP_NAME = "Sweep"
    
    # å…¨å±€ç«¯å£æ˜ å°„
    GLOBAL_PORT_MAP = {
        'S11': ('1:1', '1:1'),
        }
    
    # çº¦æŸé…ç½®
    CONSTRAINTS = [
        {
            'expression': 'mean(dB(S11))',  # å‡æ–¹è¯¯å·®æ›´å¹³æ»‘
            'target': -15,  # æ¯”ç›®æ ‡å€¼ä½3dBçš„è£•é‡
            'operator': '<', 
            'weight': 0.4,
            'freq_range': (5.9e9, 7.2e9),
            'aggregate': 'mean'
        },
        {
            'expression': 'dB(S11)',
            'target': -12,
            'operator': '<',  # æ‰€æœ‰ç«¯å£çš„æœ€å¤§åå°„ç³»æ•°å°äº-10 dB
            'weight': 0.6,
            'freq_range': (5.9e9, 6.5e9),
            'aggregate': 'max'
        },
        {
            'expression': 'dB(S11)',
            'target': -12,
            'operator': '<',  # æ‰€æœ‰ç«¯å£çš„æœ€å¤§åå°„ç³»æ•°å°äº-10 dB
            'weight': 0.6,
            'freq_range': (6.5e9, 7.2e9),
            'aggregate': 'max'
        },
    ]
    
    # å˜é‡é…ç½®
    VARIABLES = [
        {'name': 'Lp', 'bounds': (3, 30), 'unit': 'mm'},
        {'name': 'Lp_extra', 'bounds': (2, 20), 'unit': 'mm'},
        {'name': 'Wg', 'bounds': (1, 2.5), 'unit': 'mm'},
        {'name': 'Wp', 'bounds': (3, 25), 'unit': 'mm'},
        {'name': 'kLc', 'bounds': (0.2, 0.9), 'unit': 'meter'}, 
        {'name': 'kLm', 'bounds': (0.2, 0.8), 'unit': 'meter'},
        {'name': 'kWc', 'bounds': (0.2, 0.9), 'unit': 'meter'},
        {'name': 'kWm', 'bounds': (0.2, 1), 'unit': 'meter'},
    ]
    
    # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹
    optimizer = HfssAIAgentOptimizer(
        project_path=PROJECT_PATH,
        design_name=DESIGN_NAME,
        setup_name=SETUP_NAME,
        sweep_name=SWEEP_NAME,
        variables=VARIABLES,
        freq_range=(4.5e9, 8e9),
        constraints=CONSTRAINTS,
        global_port_map=GLOBAL_PORT_MAP,
        max_active_cycles=20,
        init_sample_multiplier=15,
        ei_balance=0.6,
        feature_freq_points=70,
        initial_dataset_path=r'optim_results\ai_optim_20250729-182300\dataset.npz',
    )
    
    # å¼€å§‹ä¼˜åŒ–
    best_params = optimizer.optimize()
    if best_params is not None:
        print("\n" + "="*60)
        print("ğŸ‰ğŸ‰ ä¼˜åŒ–æˆåŠŸå®Œæˆï¼æœ€ä½³å‚æ•°:")
        for i, var in enumerate(optimizer.variables):
            print(f"  {var['name']}: {best_params[i]:.4f} {var.get('unit', '')}")
        print('='*60)
    else:
        print("\nâŒ ä¼˜åŒ–æœªæ‰¾åˆ°æœ‰æ•ˆè§£")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸: {str(e)}")
        traceback.print_exc()