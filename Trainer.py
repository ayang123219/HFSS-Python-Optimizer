from ast import mod
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import os
# è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei']
plt.rcParams['axes.unicode_minus'] = False

class GPRTrainer:
    """é«˜æ–¯è¿‡ç¨‹å›å½’(GPR)æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, dataset_path, output_dir="gpr_models"):
        """
        åˆå§‹åŒ–GPRè®­ç»ƒå™¨
        
        å‚æ•°:
        dataset_path: str - æ•°æ®é›†æ–‡ä»¶è·¯å¾„(.npz)
        output_dir: str - æ¨¡å‹ä¿å­˜ç›®å½•
        """
        self.dataset_path = dataset_path
        self.output_dir = output_dir
        self.X_scaler = StandardScaler()
        self.y_scaler = StandardScaler()
        self.gpr = None
        self.X_train = None
        self.y_train = None  # æ–°å¢å±æ€§
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        if not os.path.exists(self.dataset_path):
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {self.dataset_path}")
        
        data = np.load(self.dataset_path, allow_pickle=True)
        return data
    
    def preprocess_data(self, data):
        """æ•°æ®é¢„å¤„ç†"""
        # æå–æ•°æ®å’Œç‰¹å¾å‘é‡
        X = data['X']
        y = data['feature_vectors']  # ä½¿ç”¨å±•å¹³çš„ç‰¹å¾å‘é‡
        
        # æ ‡å‡†åŒ–æ•°æ®
        X_scaled = self.X_scaler.fit_transform(X)
        y_scaled = self.y_scaler.fit_transform(y)
        
        return X_scaled, y_scaled
    
    def create_gpr_model(self):
        """åˆ›å»ºGPRæ¨¡å‹"""
        # å®šä¹‰å¤åˆæ ¸å‡½æ•°
        kernel = ConstantKernel(1.0) * RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)
        
        # åˆ›å»ºGPRæ¨¡å‹
        return GaussianProcessRegressor(
            kernel=kernel,
            n_restarts_optimizer=10,  # ä¼˜åŒ–é‡å¯æ¬¡æ•°ï¼Œæé«˜æ”¶æ•›æ¦‚ç‡
            alpha=1e-6,               # å¢åŠ æ•°å€¼ç¨³å®šæ€§
            normalize_y=True          # è‡ªåŠ¨å½’ä¸€åŒ–ç›®æ ‡å€¼
        )
    
    def train(self):
        """è®­ç»ƒGPRæ¨¡å‹"""
        # åŠ è½½æ•°æ®é›†
        data = self.load_dataset()
        # é¢„å¤„ç†æ•°æ®
        X, y = self.preprocess_data(data)

        # ä¿å­˜æ ‡å‡†åŒ–åçš„è®­ç»ƒæ•°æ®
        self.X_train = X
        self.y_train = y

        # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
        self.gpr = self.create_gpr_model()
        self.gpr.fit(X, y)
        
        # æ‰“å°è®­ç»ƒç»“æœ
        print(f"âœ… GPRæ¨¡å‹è®­ç»ƒå®Œæˆ")
        print(f"æœ€ç»ˆæ ¸å‡½æ•°: {self.gpr.kernel_}")
        print(f"å¯¹æ•°è¾¹ç¼˜ä¼¼ç„¶: {self.gpr.log_marginal_likelihood():.2f}")
        
        return self.gpr

    def predict(self, X):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„GPRæ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        å‚æ•°:
        X: np.array - è¾“å…¥å‚æ•° (n_samples, n_features)
        
        è¿”å›:
        y_pred: np.array - é¢„æµ‹çš„ç‰¹å¾å‘é‡
        y_std: np.array - é¢„æµ‹çš„æ ‡å‡†å·®ï¼ˆä¸ç¡®å®šæ€§ï¼‰
        """
        if self.gpr is None:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        # æ ‡å‡†åŒ–è¾“å…¥
        X_scaled = self.X_scaler.transform(X)
        
        # é¢„æµ‹
        y_pred_scaled, y_std_scaled = self.gpr.predict(X_scaled, return_std=True)
        
        # åæ ‡å‡†åŒ–è¾“å‡º
        y_pred = self.y_scaler.inverse_transform(y_pred_scaled)
        y_std = y_std_scaled * self.y_scaler.scale_  # è°ƒæ•´æ ‡å‡†å·®
        
        return y_pred, y_std

    def evaluate(self, X_test=None, y_test=None):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if self.gpr is None:
            raise RuntimeError("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        
        data = self.load_dataset()
        X, y = self.preprocess_data(data)
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®è¯„ä¼°
        y_pred, y_std = self.gpr.predict(X, return_std=True)
        
        # è®¡ç®—MSE
        mse = np.mean((y_pred - y)**2)
        print(f"å¹³å‡MSE: {mse:.4f}")
        
        # å¯è§†åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§
        self.plot_uncertainty(y, y_pred, y_std)
        
        return mse
    
    def plot_uncertainty(self, y_true, y_pred, y_std):
        """å¯è§†åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ - ä¿®å¤ç‰ˆ"""
        plt.figure(figsize=(12, 6))
        n_features = y_true.shape[1]
        sample_dim = min(5, n_features)
        dims = np.random.choice(n_features, sample_dim, replace=False)
        
        for i, dim in enumerate(dims):
            plt.subplot(2, 3, i+1)
            plt.scatter(range(len(y_true)), y_true[:, dim], c='k', s=5, label="çœŸå®å€¼")
            plt.plot(y_pred[:, dim], 'r-', label="é¢„æµ‹å€¼")
            
            # ä¿®å¤ï¼šä½¿ç”¨è¯¥ç»´åº¦çš„æ ‡å‡†å·®
            plt.fill_between(
                range(len(y_pred)),
                y_pred[:, dim] - 1.96 * y_std[:, dim],
                y_pred[:, dim] + 1.96 * y_std[:, dim],
                alpha=0.2,
                color='blue',
                label="95%ç½®ä¿¡åŒºé—´"
            )
            plt.title(f"ç‰¹å¾ç»´åº¦ {dim}")
            plt.legend()
        
        plt.tight_layout()
        plot_path = os.path.join(self.output_dir, "gpr_uncertainty.png")
        plt.savefig(plot_path)
        print(f"ğŸ“ŠğŸ“Š ä¸ç¡®å®šæ€§å¯è§†åŒ–å·²ä¿å­˜è‡³: {plot_path}")

    def get_train_mse(self):
        """è·å–æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„MSE"""
        if self.gpr is None:
            return float('inf')
            
        # ä½¿ç”¨è®­ç»ƒæ•°æ®è®¡ç®—MSE
        y_train_pred = self.gpr.predict(self.X_train)
        mse = np.mean((y_train_pred - self.y_train) ** 2)
        return mse

    def save_model(self):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if self.gpr is None:
            raise RuntimeError("æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹")
        
        model_path = os.path.join(self.output_dir, "gpr_model.npz")
        params = {
            'kernel': self.gpr.kernel_,
            'X_train': self.gpr.X_train_,
            'y_train': self.gpr.y_train_
        }
        np.savez(model_path, **params)
        print(f"ğŸ’¾ GPRæ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        scaler_path = os.path.join(self.output_dir, "scalers.npz")
        np.savez(scaler_path, 
                 X_scaler_mean=self.X_scaler.mean_,
                 X_scaler_scale=self.X_scaler.scale_,
                 y_scaler_mean=self.y_scaler.mean_,
                 y_scaler_scale=self.y_scaler.scale_)
        print(f"ğŸ’¾ æ ‡å‡†åŒ–å™¨å·²ä¿å­˜è‡³: {scaler_path}")

    def run(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.train()
        self.evaluate()
        self.save_model()
        return self.gpr



import tensorflow as tf
import tensorflow_probability as tfp
from tensorflow_probability import distributions as tfd 
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import numpy as np
import os
import json
import time
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.layers import Dropout

class BNNTrainer:
    """è´å¶æ–¯ç¥ç»ç½‘ç»œ(BNN)è®­ç»ƒå™¨ - é‡æ„ç‰ˆ"""
    
    def __init__(self, dataset_path, output_dir="bnn_models", 
                 n_units=20, n_samples=15, learning_rate=0.0001):
        """
        åˆå§‹åŒ–BNNè®­ç»ƒå™¨
        
        å‚æ•°:
        dataset_path: str - æ•°æ®é›†æ–‡ä»¶è·¯å¾„(.npz)
        output_dir: str - æ¨¡å‹ä¿å­˜ç›®å½•
        n_units: int - éšè—å±‚ç¥ç»å…ƒæ•°é‡
        n_samples: int - é¢„æµ‹æ—¶çš„é‡‡æ ·æ¬¡æ•°
        learning_rate: float - å­¦ä¹ ç‡
        """
        self.dataset_path = dataset_path
        self.output_dir = output_dir
        self.n_units = n_units
        self.n_samples = n_samples
        self.lr = learning_rate
        self.X_scaler = StandardScaler()
        self.y_scaler = StandardScaler()
        self.model = None
        self.train_mse = float('inf')  # å­˜å‚¨è®­ç»ƒé›†MSE
        os.makedirs(self.output_dir, exist_ok=True)
    
    def _build_bnn(self, input_dim, output_dim):
        """æ„å»ºè´å¶æ–¯ç¥ç»ç½‘ç»œç»“æ„ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
        inputs = Input(shape=(input_dim,))
        
        # ä½¿ç”¨é»˜è®¤çš„å‡å€¼åœºåéªŒåˆ†å¸ƒ
        kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn()
        
        # ç¬¬ä¸€è´å¶æ–¯å±‚
        x = tfp.layers.DenseFlipout(
            self.n_units, 
            activation='relu',
            kernel_posterior_fn=kernel_posterior_fn
        )(inputs)
        
        # ç¬¬äºŒè´å¶æ–¯å±‚
        x = tfp.layers.DenseFlipout(
            self.n_units, 
            activation='relu',
            kernel_posterior_fn=kernel_posterior_fn
        )(x)

        # æ·»åŠ Dropoutå±‚
        x = Dropout(0.3)(x)
    
        # è¾“å‡ºå±‚
        outputs = tfp.layers.DenseFlipout(
            output_dim,
            kernel_posterior_fn=kernel_posterior_fn,
            activation='softplus'
        )(x)
        
        return Model(inputs=inputs, outputs=outputs)
    
    def _negative_log_likelihood(self, y_true, y_pred):
        """è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼šè´Ÿå¯¹æ•°ä¼¼ç„¶"""
        # ä½¿ç”¨MSEä½œä¸ºæŸå¤±å‡½æ•°
        return tf.reduce_mean(tf.square(y_true - y_pred))
    
    def load_and_preprocess(self):
        data = np.load(self.dataset_path, allow_pickle=True)
        
        # éªŒè¯æ•°æ®é›†ç»“æ„
        required_fields = ['X', 'feature_vectors']
        for field in required_fields:
            if field not in data:
                raise ValueError(f"æ•°æ®é›†ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
        
        X = data['X']
        y = data['feature_vectors']
        
        print(f"æ•°æ®é›†éªŒè¯é€šè¿‡: è¾“å…¥ç»´åº¦={X.shape}, è¾“å‡ºç»´åº¦={y.shape}")
        
        # æ ‡å‡†åŒ–æ•°æ®
        X_scaled = self.X_scaler.fit_transform(X)
        y_scaled = self.y_scaler.fit_transform(y)
        
        return X_scaled, y_scaled
    
    def train(self, epochs=800, batch_size=32):
        X, y = self.load_and_preprocess()
        input_dim = X.shape[1]
        output_dim = y.shape[1]
        
        self.model = self._build_bnn(input_dim, output_dim)
        
        # ä½¿ç”¨å†…ç½®MSEæŸå¤±æ›¿ä»£è‡ªå®šä¹‰æŸå¤±
        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr), 
                           loss='mse')
        
        history = self.model.fit(
            X, y,
            epochs=epochs,
            batch_size=batch_size,
            verbose=0,  # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
            callbacks=[
                tf.keras.callbacks.EarlyStopping(patience=15,monitor='val_loss', restore_best_weights=True),
                tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=7, min_lr=1e-6)
            ],
            validation_split=0.2
        )
        
        self.train_mse = history.history['loss'][-1]
        print(f"âœ… BNNè®­ç»ƒå®Œæˆ | æœ€ç»ˆè®­ç»ƒæŸå¤±: {self.train_mse:.4f}")
        self.save_training_history(history)
        return history
    
    def _calculate_train_mse(self, X, y):
        """è®¡ç®—è®­ç»ƒé›†ä¸Šçš„MSE"""
        if self.model is None:
            return float('inf')
            
        # ä½¿ç”¨æ¨¡å‹é¢„æµ‹è®­ç»ƒé›†
        y_pred = self.model(X)  # ç›´æ¥è·å–é¢„æµ‹å€¼
        
        # è®¡ç®—MSE - ä½¿ç”¨TensorFlowçš„MSEå‡½æ•°
        mse = tf.keras.losses.MSE(y, y_pred)
        return tf.reduce_mean(mse).numpy()
    
    def get_train_mse(self):
        """è·å–è®­ç»ƒé›†MSEï¼ˆç”¨äºä¸»åŠ¨å­¦ä¹ æ”¶æ•›åˆ¤æ–­ï¼‰"""
        return self.train_mse
    
    def predict(self, X):
        """é¢„æµ‹ç‰¹å¾å‘é‡åŠä¸ç¡®å®šæ€§"""
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        # æ£€æŸ¥è¾“å…¥ç»´åº¦
        if len(X.shape) == 1:
            X = X.reshape(1, -1)
        
        # æ ‡å‡†åŒ–è¾“å…¥
        X_scaled = self.X_scaler.transform(X)
        
        # å¤šæ¬¡é‡‡æ ·é¢„æµ‹
        y_pred_samples = []
        for _ in range(self.n_samples):
            y_pred = self.model(X_scaled).numpy()
            y_pred_samples.append(y_pred)
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        y_pred_samples = np.array(y_pred_samples)
        y_pred_mean = np.mean(y_pred_samples, axis=0)
        y_pred_std = np.std(y_pred_samples, axis=0)
        
        # åæ ‡å‡†åŒ–
        y_pred = self.y_scaler.inverse_transform(y_pred_mean)
        y_std = y_pred_std * self.y_scaler.scale_
        
        return y_pred, y_std
    
    def save_training_history(self, history):
        """ä¿å­˜BNNè®­ç»ƒå†å²"""
        if history is None:
            print("âš ï¸ æ— è®­ç»ƒå†å²å¯ä¿å­˜")
            return
            
        # åˆ›å»ºå†å²æ•°æ®
        training_history = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "epochs": list(range(1, len(history.history['loss']) + 1)),
            "loss": history.history['loss'],
            "val_loss": history.history.get('val_loss', [])
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        history_path = os.path.join(self.output_dir, "bnn_training_history.json")
        with open(history_path, 'w') as f:
            json.dump(training_history, f, indent=2)
        
        print(f"ğŸ“ BNNè®­ç»ƒå†å²å·²ä¿å­˜è‡³: {history_path}")
        return training_history
        
    def get_model_summary(self):
        if self.model is None:
            return {}
        
        return {
            "model_type": "BNN",
            "input_dim": int(self.model.input_shape[1]),  # æ˜¾å¼è½¬æ¢
            "output_dim": int(self.model.output_shape[1]),  # æ˜¾å¼è½¬æ¢
            "params_count": int(self.model.count_params()),  # æ˜¾å¼è½¬æ¢
            "train_mse": float(self.train_mse)  # æ˜¾å¼è½¬æ¢
        }

    def save_model(self):
        """ä¿å­˜æ¨¡å‹åŠé¢„å¤„ç†å™¨"""
        if self.model is None:
            raise RuntimeError("æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹")
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(self.output_dir, "bnn_model.h5")
        self.model.save(model_path)
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        scaler_path = os.path.join(self.output_dir, "scalers.npz")
        np.savez(scaler_path,
                 X_scaler_mean=self.X_scaler.mean_,
                 X_scaler_scale=self.X_scaler.scale_,
                 y_scaler_mean=self.y_scaler.mean_,
                 y_scaler_scale=self.y_scaler.scale_)
        
        print(f"ğŸ’¾ BNNæ¨¡å‹ä¿å­˜è‡³: {model_path}")
        print(f"ğŸ’¾ æ ‡å‡†åŒ–å™¨ä¿å­˜è‡³: {scaler_path}")
    
    def run(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        history = self.train()
        self.save_model()
        return self.model